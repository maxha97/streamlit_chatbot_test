{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: curl: command not found\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
      "Need to get 2129 kB of archives.\n",
      "After this operation, 7662 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2129 kB]\n",
      "Fetched 2129 kB in 2s (947 kB/s)                    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 33396 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
      "Unpacking git-lfs (2.3.4-1) ...\n",
      "Setting up git-lfs (2.3.4-1) ...\n",
      "W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/elastic-7.x.list:1 and /etc/apt/sources.list.d/elastic-7.x.list:2\n",
      "W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/elastic-7.x.list:1 and /etc/apt/sources.list.d/elastic-7.x.list:2\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
      "Git LFS initialized.\n",
      "Cloning into 'kogpt2'...\n",
      "remote: Enumerating objects: 52, done.\u001b[K\n",
      "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
      "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
      "remote: Total 52 (delta 20), reused 0 (delta 0), pack-reused 0\n",
      "Unpacking objects: 100% (52/52), done.\n",
      "Filtering content: 100% (2/2), 959.93 MiB | 119.44 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/taeminlee/kogpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50000, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SentencePieceBPETokenizer(\"./kogpt2/vocab.json\", \"./kogpt2/merges.txt\")\n",
    "\n",
    "config = GPT2Config(vocab_size=50000)\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "model_dir = './kogpt2/pytorch_model.bin'\n",
    "\n",
    "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['▁이순', '신은', '▁조선', '▁중기의', '▁무신', '이다', '.']\n",
      "[10925, 6647, 1117, 40249, 39793, 128, 47440]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode('이순신은 조선 중기의 무신이다.', add_special_tokens=True)\n",
    "print(tokenized_text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Chatbot_data'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
      "remote: Total 57 (delta 21), reused 6 (delta 3), pack-reused 18\u001b[K\n",
      "Unpacking objects: 100% (57/57), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/songys/Chatbot_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./Chatbot_data/ChatbotData.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "added_special_token_num = tokenizer.add_special_tokens(['<s>', '</s>'])\n",
    "print(added_special_token_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "print(pad_id)\n",
    "tokenizer.enable_padding(pad_id=pad_id, pad_token=\"<pad>\")\n",
    "tokenizer.enable_truncation(max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, file_path):\n",
    "        self.data = []\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_data(self):\n",
    "        raw_data = pd.read_csv(self.file_path)\n",
    "        train_data = '<s>'+raw_data['Q']+'</s>'+'<s>'+raw_data['A']+'</s>'\n",
    "        #<s>안녕하세요</s><s> -> 네, 안녕하세요</s>\n",
    "        tokenized_train_data = tokenizer.encode_batch(train_data)\n",
    "        for single_data in tokenized_train_data:\n",
    "            self.data.append(torch.tensor(single_data.ids).unsqueeze(0))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChatDataset(tokenizer=tokenizer, file_path='./Chatbot_data/ChatbotData.csv')\n",
    "train_dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 04:14:11.997368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-19 04:14:11.997407: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.0  train (0/2956)  loss = 2.99108  avg_loss = 2.99108\n",
      "epoch no.0  train (200/2956)  loss = 1.25670  avg_loss = 1.47491\n",
      "epoch no.0  train (400/2956)  loss = 1.21854  avg_loss = 1.29296\n",
      "epoch no.0  train (600/2956)  loss = 0.87010  avg_loss = 1.22314\n",
      "epoch no.0  train (800/2956)  loss = 1.17217  avg_loss = 1.19936\n",
      "epoch no.0  train (1000/2956)  loss = 1.12726  avg_loss = 1.20836\n",
      "epoch no.0  train (1200/2956)  loss = 1.51246  avg_loss = 1.17679\n",
      "epoch no.0  train (1400/2956)  loss = 1.01038  avg_loss = 1.16660\n",
      "epoch no.0  train (1600/2956)  loss = 1.14180  avg_loss = 1.14344\n",
      "epoch no.0  train (1800/2956)  loss = 0.93434  avg_loss = 1.17536\n",
      "epoch no.0  train (2000/2956)  loss = 1.00604  avg_loss = 1.16753\n",
      "epoch no.0  train (2200/2956)  loss = 0.94353  avg_loss = 1.11635\n",
      "epoch no.0  train (2400/2956)  loss = 1.37008  avg_loss = 1.12345\n",
      "epoch no.0  train (2600/2956)  loss = 1.15827  avg_loss = 1.12674\n",
      "epoch no.0  train (2800/2956)  loss = 0.87587  avg_loss = 1.11625\n",
      "epoch no.1  train (0/2956)  loss = 0.85589  avg_loss = 1.08910\n",
      "epoch no.1  train (200/2956)  loss = 1.08313  avg_loss = 0.97470\n",
      "epoch no.1  train (400/2956)  loss = 0.89691  avg_loss = 0.95232\n",
      "epoch no.1  train (600/2956)  loss = 1.01054  avg_loss = 0.97296\n",
      "epoch no.1  train (800/2956)  loss = 0.72070  avg_loss = 0.96077\n",
      "epoch no.1  train (1000/2956)  loss = 0.81262  avg_loss = 0.95161\n",
      "epoch no.1  train (1200/2956)  loss = 1.25980  avg_loss = 0.92931\n",
      "epoch no.1  train (1400/2956)  loss = 0.64961  avg_loss = 0.94061\n",
      "epoch no.1  train (1600/2956)  loss = 0.93594  avg_loss = 0.97404\n",
      "epoch no.1  train (1800/2956)  loss = 0.90847  avg_loss = 0.93873\n",
      "epoch no.1  train (2000/2956)  loss = 0.82417  avg_loss = 0.92306\n",
      "epoch no.1  train (2200/2956)  loss = 1.01184  avg_loss = 0.91285\n",
      "epoch no.1  train (2400/2956)  loss = 0.91179  avg_loss = 0.92078\n",
      "epoch no.1  train (2600/2956)  loss = 0.84515  avg_loss = 0.92432\n",
      "epoch no.1  train (2800/2956)  loss = 1.34195  avg_loss = 0.93688\n",
      "epoch no.2  train (0/2956)  loss = 0.64223  avg_loss = 0.90780\n",
      "epoch no.2  train (200/2956)  loss = 0.75189  avg_loss = 0.76536\n",
      "epoch no.2  train (400/2956)  loss = 0.87029  avg_loss = 0.76292\n",
      "epoch no.2  train (600/2956)  loss = 1.05732  avg_loss = 0.74596\n",
      "epoch no.2  train (800/2956)  loss = 0.72055  avg_loss = 0.77886\n",
      "epoch no.2  train (1000/2956)  loss = 0.54874  avg_loss = 0.78281\n",
      "epoch no.2  train (1200/2956)  loss = 0.79008  avg_loss = 0.77539\n",
      "epoch no.2  train (1400/2956)  loss = 0.87919  avg_loss = 0.79475\n",
      "epoch no.2  train (1600/2956)  loss = 0.71630  avg_loss = 0.76956\n",
      "epoch no.2  train (1800/2956)  loss = 0.52138  avg_loss = 0.77317\n",
      "epoch no.2  train (2000/2956)  loss = 0.60303  avg_loss = 0.78442\n",
      "epoch no.2  train (2200/2956)  loss = 0.70692  avg_loss = 0.76670\n",
      "epoch no.2  train (2400/2956)  loss = 0.98211  avg_loss = 0.79509\n",
      "epoch no.2  train (2600/2956)  loss = 0.96505  avg_loss = 0.80001\n",
      "epoch no.2  train (2800/2956)  loss = 0.82897  avg_loss = 0.78432\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4, correct_bias=True)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "avg_loss = (0.0, 0.0)\n",
    "for epoch in range(epochs):\n",
    "    count=0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.transpose(1,0)\n",
    "        data = data.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "        \n",
    "        outputs = model(data, labels=data)\n",
    "        loss, logits = outputs[:2]\n",
    "        loss = loss.to('cuda')\n",
    "        loss.backward()\n",
    "        avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
    "        optimizer.step()\n",
    "        if count % 200 == 0:\n",
    "            print('epoch no.{0}  train ({1}/{2})  loss = {3:.5f}  avg_loss = {4:.5f}' . format(epoch, count, len(data_loader), loss, avg_loss[0] / avg_loss[1]))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chitchat_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(text):\n",
    "    text = '<s>'+text+'</s><s>'\n",
    "    return torch.tensor(tokenizer.encode(text).ids).unsqueeze(0).to('cuda')\n",
    "\n",
    "def decoding(ids):\n",
    "    return tokenizer.decode_batch(ids)\n",
    "\n",
    "tokenizer.no_padding()\n",
    "tokenizer.no_truncation()\n",
    "\n",
    "e_s = tokenizer.token_to_id('</s>')\n",
    "unk = tokenizer.token_to_id('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(text):\n",
    "    text = '<s>'+text+'</s><s>'\n",
    "    return torch.tensor(tokenizer.encode(text).ids).unsqueeze(0).to('cuda')\n",
    "\n",
    "def decoding(ids):\n",
    "    return tokenizer.decode_batch(ids)\n",
    "\n",
    "tokenizer.no_padding()\n",
    "tokenizer.no_truncation()\n",
    "\n",
    "e_s = tokenizer.token_to_id('</s>')\n",
    "unk = tokenizer.token_to_id('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input_sent):\n",
    "    input_ids = encoding(input_sent)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        num_return_sequences=5,\n",
    "        do_sample=True, \n",
    "        max_length=128, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        eos_token_id=e_s,\n",
    "        early_stopping=True,\n",
    "        bad_words_ids=[[unk]]\n",
    "    )\n",
    "\n",
    "    decoded_result = decoding(sample_outputs.tolist())\n",
    "    for result in decoded_result:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "def encoding(text):\n",
    "    text = '<s>'+text+'</s><s>'\n",
    "    return torch.tensor(tokenizer.encode(text).ids).unsqueeze(0).to('cuda')\n",
    "\n",
    "def decoding(ids, tokenizer):\n",
    "    return tokenizer.decode_batch(ids)\n",
    "\n",
    "def get_answer(input_sent, e_s, unk):\n",
    "    input_ids = encoding(input_sent)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        num_return_sequences=5,\n",
    "        do_sample=True, \n",
    "        max_length=128, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        eos_token_id=e_s,\n",
    "        early_stopping=True,\n",
    "        bad_words_ids=[[unk]]\n",
    "    )\n",
    "\n",
    "    decoded_result = decoding(sample_outputs.tolist())\n",
    "    for result in decoded_result:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceBPETokenizer(\"./kogpt2/vocab.json\", \"./kogpt2/merges.txt\")\n",
    "tokenizer.no_padding()\n",
    "tokenizer.no_truncation()\n",
    "e_s = tokenizer.token_to_id('</s>')\n",
    "unk = tokenizer.token_to_id('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = torch.load('/opt/ml/Kogpt2_chatbot/chitchat_model.bin', map_location='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input_sent, e_s, unk):\n",
    "    input_ids = encoding(input_sent)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        num_return_sequences=5,\n",
    "        do_sample=True, \n",
    "        max_length=128, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        eos_token_id=e_s,\n",
    "        early_stopping=True,\n",
    "        bad_words_ids=[[unk]]\n",
    "    )\n",
    "\n",
    "    decoded_result = decoding(sample_outputs.tolist())\n",
    "    for result in decoded_result:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python ('seongjin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
